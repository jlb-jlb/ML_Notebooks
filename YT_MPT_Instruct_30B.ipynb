{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlb-jlb/ML_Notebooks/blob/main/YT_MPT_Instruct_30B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VJ7lfTYz0qu",
        "outputId": "9fd05f3e-b191-4fff-848f-7b2f4d3beb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate xformers einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWAVMJcUVCyu",
        "outputId": "09c78c44-14a4-445c-a68e-a5eb17c7057f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python\n",
            "  Cloning https://github.com/vchiley/triton.git (to revision triton_pre_mlir) to /tmp/pip-install-dr8z8n7k/triton-pre-mlir_fd17c7084f33440896262f867f4ce768\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/vchiley/triton.git /tmp/pip-install-dr8z8n7k/triton-pre-mlir_fd17c7084f33440896262f867f4ce768\n",
            "  Running command git checkout -b triton_pre_mlir --track origin/triton_pre_mlir\n",
            "  Switched to a new branch 'triton_pre_mlir'\n",
            "  Branch 'triton_pre_mlir' set up to track remote branch 'triton_pre_mlir' from 'origin'.\n",
            "  Resolved https://github.com/vchiley/triton.git to commit 2dd3b957698a39bbca615c02a447a98482c144a3\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (3.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (3.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (2.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (11.7.4.91)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (11.7.91)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (2.14.3)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (2.0.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (0.40.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->triton-pre-mlir@ git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python) (1.3.0)\n",
            "Building wheels for collected packages: triton-pre-mlir\n",
            "  Building wheel for triton-pre-mlir (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for triton-pre-mlir: filename=triton_pre_mlir-2.0.0-cp310-cp310-linux_x86_64.whl size=15415803 sha256=811c4c9d225b2f3bb32bc6586a943d22d395f6e1c31f9ca22573f21d8a642d16\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5et8mkrw/wheels/c8/ca/ac/0732eb54df0cd4b279d82c8a34369f0c042f42ca7dcc0002ef\n",
            "Successfully built triton-pre-mlir\n",
            "Installing collected packages: triton-pre-mlir\n",
            "Successfully installed triton-pre-mlir-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "ec35c8fb-8b27-4bfc-b99f-36341a8269f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 24 09:59:44 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA A100-SXM...  Off  | 00000000:00:05.0 Off |                    0 |\n",
            "| N/A   33C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "name = 'mosaicml/mpt-30b-instruct'\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\n",
        "\n",
        "\n",
        "# model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "#   'mosaicml/mpt-30b-instruct',\n",
        "#   torch_dtype=torch.bfloat16,\n",
        "#   device_map='auto',\n",
        "# #   load_in_8bit=True,\n",
        "#   trust_remote_code=True\n",
        "# )\n",
        "\n",
        "\n",
        "## for 8bit use load in 8bit\n",
        "config = transformers.AutoConfig.from_pretrained(name,\n",
        "                                                 trust_remote_code=True)\n",
        "# config.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\n",
        "config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
        "config.max_seq_len = 16384\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "  name,\n",
        "  config=config,\n",
        "  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n",
        "  trust_remote_code=True,\n",
        "  device_map='auto',\n",
        "    # load_in_8bit=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "SsCR0FZHL1pg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "e519161298c1403083cc8668064abcf8",
            "1f5425ca85374bd0b0f505e9d5fa3d96",
            "63705f9227f741f4a2008eb7e66fe357",
            "6fbc58cb77cf4d1a82e9f28cc95ffaaa",
            "9fc95d12b7a44798bfbfbd3abbc12caf",
            "2e382aa10c1b4365b58e26415973a324",
            "48f53d1afc1049dda7ed1f31ba0a6246",
            "96b6d06203644a0a8aee02b37622331e",
            "09358c12124f4a17b1a234991417255c",
            "82d724ed818b428185c4fc5db50bdb90",
            "9cb38479c5814a189182206fce98e859"
          ]
        },
        "outputId": "a5656ea7-e12b-40e1-a7a2-8a9b83dad750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are using config.init_device='cuda:0', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e519161298c1403083cc8668064abcf8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8IS_kfwtE1uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b233886f-78e5-4b96-c507-16d67b123da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 24 10:02:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    58W / 400W |  28443MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA A100-SXM...  Off  | 00000000:00:05.0 Off |                    0 |\n",
            "| N/A   33C    P0    58W / 400W |  30083MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "    inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n",
        "    outputs = model.generate(**inputs,\n",
        "                             max_new_tokens=100,\n",
        "                             eos_token_id=tokenizer.eos_token_id,\n",
        "                            pad_token_id=tokenizer.pad_token_id,\n",
        "                            )\n",
        "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCTN2-koDuED",
        "outputId": "557dd35c-e4f2-4799-95e1-76f9649cc272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Here is a recipe for vegan banana bread:\\n\\nIngredients:\\n\\n2 cups flour\\n\\n1/2 tsp baking powder\\n\\n1/2 tsp baking soda\\n\\n1/2 tsp salt\\n\\n1/2 cup sugar\\n\\n1/2 cup melted vegan butter\\n\\n1/2 cup mashed banana (about 1 large banana)\\n\\n1/2 cup plain non-dairy milk\\n\\n1 tsp vanilla extract\\n\\nPreheat oven to 350 degrees F.\\n\\nGrease a loaf pan and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "\n",
        "# or using the HF pipeline\n",
        "pipe = pipeline('text-generation',\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device_map='auto',\n",
        "                # device='cuda:0'\n",
        "                )\n",
        "\n",
        "\n",
        "with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "    print(\n",
        "        pipe('Here is a recipe for vegan banana bread:\\n',\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            use_cache=True))\n"
      ],
      "metadata": {
        "id": "me2EOy-rL1s0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d8c0b6-2640-4c4a-b63a-0b94ef6cb36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'MPTForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Here is a recipe for vegan banana bread:\\nCombine the flour, sugar, baking soda, and salt in a medium bowl and set aside. In a separate bowl or the bowl of a stand mixer, mash the bananas with a fork or whisk in the oil, sugar, vanilla, and milk until smooth. Add the flour mixture to the banana mixture and stir to combine. Pour into the prepared pan# Copyright (c) Microsoft Corporation.\\n# Licensed under the MIT License.\\n\\nimport time\\nimport numpy as np\\nfrom'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dUMCB_kiTom"
      },
      "source": [
        "### The prompt & response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo-FSysZiVkA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "def get_prompt(instruction):\n",
        "    prompt_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\"\n",
        "    return prompt_template.format(instruction=instruction)\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "def generate(text):\n",
        "    prompt = get_prompt(text)\n",
        "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "        response = pipe(prompt,\n",
        "                        max_new_tokens=256,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_p =0.95,\n",
        "                        top_k =  50,\n",
        "                        eos_token_id = 0,\n",
        "                        use_cache=True)[0]['generated_text']\n",
        "    return response\n",
        "\n",
        "def generate(text):\n",
        "    prompt = get_prompt(text)\n",
        "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "        outputs = model.generate(**inputs,\n",
        "                                 max_new_tokens=512,\n",
        "                                 eos_token_id=tokenizer.eos_token_id,\n",
        "                                 pad_token_id=tokenizer.pad_token_id,\n",
        "                                 )\n",
        "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
        "        final_outputs = cut_off_text(final_outputs, '<|endoftext|>')\n",
        "        final_outputs = remove_substring(final_outputs, prompt)\n",
        "\n",
        "    return final_outputs#, outputs\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'What are the differences between alpacas, vicunas and llamas?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qHrby3n0109",
        "outputId": "1b7b7f12-ce3d-4502-db84-2c423d6aebcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpacas, vicunas and llamas are all related species of New World camelids. They are all domesticated\n",
            "and are used for a variety of purposes including as pets, as pack animals, for their fleece, and as\n",
            "meat.  Alpacas are a domesticated species native to the Andes mountains of South America. They are\n",
            "generally smaller than llamas, with an average height at the withers of about 1.1 m (3.6 ft) and an\n",
            "average weight of about 90 kg (200 lb). Alpacas are generally hairy with a soft, fluffy fleece that\n",
            "is used for textiles.  Vicunas are a wild relative of the domesticated alpaca. They are generally\n",
            "similar in appearance to alpacas, but are generally larger in size, with an average height at the\n",
            "withers of about 1.4 m (4.6 ft) and an average weight of about 120 kg (260 lb). Vicunas generally\n",
            "have a coarser, less soft fleece than alpacas.  Llamas are a domesticated species native to the\n",
            "Andes mountains of South America. They are generally larger than alpacas, with an average height at\n",
            "the withers of about 1.8 m (5.9 ft) and an average weight of about 150 kg (330 lb). Llamas generally\n",
            "have a coarser, less soft fleece than alpacas.\n",
            "\n",
            "\n",
            "CPU times: user 27 s, sys: 55.4 ms, total: 27.1 s\n",
            "Wall time: 27 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "angnwW9HG4Hv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd55a80-23fe-4dd4-f634-2260d85330af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "London is the capital of England\n",
            "\n",
            "\n",
            "CPU times: user 508 ms, sys: 2.87 ms, total: 511 ms\n",
            "Wall time: 508 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'What is the capital of England?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGCCFto2G4Jk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ab7e65-77ee-44a2-fc4f-afa0c6c456a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Sam,  I hope this email finds you well. I am a big fan of yours and what you stand for. I have\n",
            "been following OpenAI and your other ventures for a long time.  I am writing to you because I\n",
            "believe you have the power to make a difference in the world. I am a big believer in AI and think\n",
            "that OpenAI is on the right path to making a big impact in the world. I also think that GPT-4 is a\n",
            "huge leap forward in AI and will be a game changer in many industries.  I believe that GPT-4 should\n",
            "be open sourced for the following reasons:  1. It will help accelerate AI research and development.\n",
            "2. It will help democratize AI and make it more accessible to developers. 3. It will help create\n",
            "more AI applications and services. 4. It will help improve existing AI models. 5. It will help\n",
            "create new AI models. 6. It will help create more AI-powered products and services. 7. It will help\n",
            "create more jobs in the AI industry. 8. It will help improve existing products and services. 9. It\n",
            "will help create new products and services. 10. It will help improve the state of the world.  I hope\n",
            "you consider my request and open source GPT-4. I am sure you will get a lot of requests to do so,\n",
            "but I hope you will consider mine. I am sure you have thought about this a lot and have good reasons\n",
            "to not open source GPT-4, but I hope you will consider the reasons I listed above.  Thank you for\n",
            "your time and consideration. I am looking forward to seeing what OpenAI will do next.  Best,  Your\n",
            "biggest fan\n",
            "\n",
            "\n",
            "CPU times: user 36.9 s, sys: 58 ms, total: 37 s\n",
            "Wall time: 36.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Write an email to Sam Altman giving reasons to open source GPT-4'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9uswqYmG4LZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4857cf4f-c9d9-43eb-9728-26e53dfbec9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have processed many episodes of The Simpsons. I have a 98.3% confidence that Homer is the father\n",
            "of the Simpson family, a nuclear family which is the main focus of the show. He is married to Marge\n",
            "and they have three children, Bart, Lisa and Maggie. I have a 95.2% confidence that his favorite\n",
            "food is donuts, a deep fried sweetened ring-shaped flour-based food.\n",
            "\n",
            "\n",
            "CPU times: user 6.52 s, sys: 20.6 ms, total: 6.54 s\n",
            "Wall time: 6.52 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'As an AI do you like the Simpsons? What do you know about Homer?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYM0_ryUG4NO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3472e2e8-526d-4e8f-8b29-d153292faff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homer is the father of the Simpson family.  He works at the Springfield Nuclear Power Plant as an\n",
            "operator.  He is married to Marge and they have three children, Bart, Lisa, and Maggie.  Homer is\n",
            "known for his love of food, especially donuts.  He is not very smart and often makes stupid\n",
            "decisions.  He is also lazy and likes to watch TV.  He is the opposite of his wife Marge who is very\n",
            "smart and organized.  Homer is best friends with Barney Gumble.\n",
            "\n",
            "\n",
            "CPU times: user 8.19 s, sys: 11.5 ms, total: 8.2 s\n",
            "Wall time: 8.17 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Homer on the TV show the simpsons'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Homer on the TV show the simpsons in depth'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFQ_jT0iMc4O",
        "outputId": "82974bb6-b69e-4967-d293-1ebd8b20f390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homer Jay Simpson is the father of the family on the show.  He is a lazy, overweight, ignorant,\n",
            "alcoholic slob who loves to eat and drink.  He is married to Marge and they have three children,\n",
            "Bart, Lisa, and Maggie.  Homer is the complete opposite of his wife Marge, and often finds himself\n",
            "in trouble because of his stupidity.  He works at the Springfield Nuclear Power Plant as a safety\n",
            "inspector, but he is constantly getting fired because of his stupidity.  He is a huge fan of donuts,\n",
            "beer, and television.  He is also a very bad role model for his children, but he loves his family\n",
            "very much.  He is the main character on the show, and is the most popular character on the show.\n",
            "\n",
            "\n",
            "CPU times: user 12.3 s, sys: 22.6 ms, total: 12.4 s\n",
            "Wall time: 12.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "prompt = 'Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmbDQ82vMPYy",
        "outputId": "141822f7-afb6-458a-f064-5e025dd6fdec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before buying 6 more apples, the cafeteria had 23 - 20 = 3 apples left. After buying 6 more apples,\n",
            "the cafeteria has 3 + 6 = 9 apples.\n",
            "\n",
            "\n",
            "CPU times: user 2.75 s, sys: 4.94 ms, total: 2.76 s\n",
            "Wall time: 2.75 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'Answer the following yes\\/no question by reasoning step-by-step. \\n Can you write a whole Haiku in a single tweet?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHKCo6VXNByX",
        "outputId": "1f2c9751-25c6-4519-b038-1d4fd0c905e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Haiku is a form of Japanese poetry consisting of three lines with 17 syllables. The first line has 5\n",
            "syllables, the second line has 7 syllables, and the third line has 5 syllables. A tweet, by\n",
            "contrast, has a strict character limit of 280 characters. It is not possible to write a Haiku in a\n",
            "single tweet.\n",
            "\n",
            "\n",
            "CPU times: user 5.17 s, sys: 16.7 ms, total: 5.19 s\n",
            "Wall time: 5.17 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Harry Potter and studying at Hogwarts?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsn1buh6NTie",
        "outputId": "c1ebd024-66fd-4e0c-cc75-fff7306ffb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry Potter is the main character in a book series written by J.K. Rowling. The books are about a\n",
            "boy who learns he is a wizard, and is invited to attend the Hogwarts School of Witchcraft and\n",
            "Wizardry. The books follow Harry's adventures in the wizarding world, and his battles against the\n",
            "dark wizard Lord Voldemort. The books are very popular among children and adults, and have sold over\n",
            "400 million copies.\n",
            "\n",
            "\n",
            "CPU times: user 6.82 s, sys: 14.2 ms, total: 6.84 s\n",
            "Wall time: 6.81 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"Convert the following to JSON\n",
        "\n",
        "name: John\n",
        "age: 30\n",
        "address:\n",
        "street: 123 Main Street\n",
        "city: San Fransisco\n",
        "state: CA\n",
        "zip: 94101\n",
        "\"\"\"\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7qsYoDUT57D",
        "outputId": "bff3ee51-60da-45d0-80d7-5cdf58950499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\":\"John\",\"age\":30,\"address\":{\"street\":\"123 Main Street\",\"city\":\"San\n",
            "Fransisco\",\"state\":\"CA\",\"zip\":\"94101\"}}\n",
            "\n",
            "\n",
            "CPU times: user 2.6 s, sys: 5.41 ms, total: 2.61 s\n",
            "Wall time: 2.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"How are you today?\"\"\"\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp_lI2mkrIls",
        "outputId": "a8f6be94-61e5-47e8-8db6-c9d43b8c4bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am doing well, and myself? I am doing well.\n",
            "\n",
            "\n",
            "CPU times: user 1 s, sys: 265 µs, total: 1 s\n",
            "Wall time: 997 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"Write me a short plan for a 3 day trip to London\"\"\"\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfvXtlq1rNu9",
        "outputId": "73f866cb-4def-47e6-88f3-f522634a36d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Day 1: Start your day with a traditional English breakfast. Then visit Buckingham Palace to see the\n",
            "changing of the guard. In the afternoon, visit the Tower of London and the Crown Jewels. For dinner,\n",
            "enjoy fish and chips. End your day with a play in the West End.  Day 2: Start your day with a free\n",
            "guided tour of the Houses of Parliament and Big Ben. Then visit Westminster Abbey, the coronation\n",
            "site of English monarchs. In the afternoon, take a tour of the British Museum, one of the world's\n",
            "largest museums. For dinner, enjoy a traditional English pub meal. End your day with a walk through\n",
            "Covent Garden.  Day 3: Start your day with a free guided tour of St. Paul's Cathedral. Then visit\n",
            "the National Gallery, one of the world's greatest collections of Western European painting. For\n",
            "dinner, enjoy a traditional afternoon tea. End your day with a visit to the London Eye for a\n",
            "panoramic view of the city.\n",
            "\n",
            "\n",
            "CPU times: user 15.9 s, sys: 32.8 ms, total: 15.9 s\n",
            "Wall time: 15.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxtk83DVyR48"
      },
      "outputs": [],
      "source": [
        "article = \"\"\"\n",
        "Content moderators under Sama, Meta’s content review sub-contractor in Africa, earlier today picketed at the company’s headquarters in Kenya demanding April salary, while urging it to observe the court orders that barred it from conducting mass layoffs.\n",
        "\n",
        "The demonstrations came after Sama, in an email, instructed moderators to clear with the company by May 11, a move the employees say is against the existing court orders.\n",
        "\n",
        "The 184 moderators sued Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March, and Majorel, the social media giant’s new partner in Africa, for blacklisting on instruction by Meta.\n",
        "\n",
        "\n",
        "The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy, and Meta from engaging Majorel, which was also instructed to refrain from blacklisting the moderators. Sama was directed to continue reviewing content on Meta’s platforms, and to be its sole provider in Africa pending determination of the case. However, Sama sent the moderators on compulsory leave in April saying it had no work for them as its contract with Meta had expired.\n",
        "\n",
        "Sama told TechCrunch that it had sent the notice “to staff whose contract had expired to go through our regular clearance process. This clearance process involves the return of company equipment to make sure that all final dues can be paid without deduction for that equipment, in accordance with Kenyan law.”\n",
        "\n",
        "It said the moderators’ contracts had ended in March after its deal with Meta expired, saying that it was only processing the moderators final dues.\n",
        "\n",
        "“We understand our former employees’ frustration because they were led by others to believe that they would all receive salary indefinitely while on leave, but that is not what the court dictated,” said Sama.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"Please summarize this article:\\n\" + article\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CEQDmM4xL-J",
        "outputId": "4e9cdabe-d582-4274-aa96-f7af0afd11a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sama, Meta’s content review subcontractor in Africa, instructed its content moderators to clear\n",
            "their separation from the company by May 11, a move the employees say is against the existing court\n",
            "orders. The moderators are picketing at the company’s headquarters in Kenya, demanding April salary\n",
            "and urging Sama to observe the court orders that barred it from conducting mass layoffs.  The\n",
            "demonstrations came after Sama, in an email, instructed moderators to clear with the company by May\n",
            "11, a move the employees say is against the existing court orders.  The 184 moderators sued Sama for\n",
            "allegedly laying them off unlawfully, after it wound down its content review arm in March, and\n",
            "Majorel, the social media giant’s new partner in Africa, for blacklisting on instruction by Meta.\n",
            "The court issued a temporary injunction on March 21 barring Sama from effecting any form of\n",
            "redundancy, and Meta from engaging Majorel, which was also instructed to refrain from blacklisting\n",
            "the moderators. Sama was directed to continue reviewing content on Meta’s platforms, and to be its\n",
            "sole provider in Africa pending determination of the case. However, Sama sent the moderators on\n",
            "compulsory leave in April saying it had no work for them as its contract with Meta had expired.\n",
            "Sama told TechCrunch that it had sent the notice “to staff whose contract had expired to go through\n",
            "our regular clearance process. This clearance process involves the return of company equipment to\n",
            "make sure that all final dues can be paid without deduction for that equipment, in accordance with\n",
            "Kenyan law.”  It said the moderators’ contracts had ended in March after its deal with Meta expired,\n",
            "saying that it was only processing the moderators’ final dues.  “We understand our former employees’\n",
            "frustration because they were led by others to believe that they would all receive salary\n",
            "indefinitely while on leave, but that is not what the court dictated,” said Sama.\n",
            "\n",
            "\n",
            "CPU times: user 1min 30s, sys: 247 ms, total: 1min 31s\n",
            "Wall time: 1min 30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"Please extract the key info as bullet points for this article:\\n\" + article\n",
        "generated_text = generate(prompt)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzko3r6mxU9-",
        "outputId": "e3a5ace3-5c19-4a1e-b2f7-1d55da8cf667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the key points from the article:\n",
            "\n",
            "1. Content moderators under Sama, Meta’s content review sub-contractor in Africa, picketed at the company’s headquarters in Kenya today.\n",
            "\n",
            "2. The moderators are demanding April salary and are urging Sama to observe the court orders that barred it from conducting mass layoffs.\n",
            "\n",
            "3. The demonstrations came after Sama instructed moderators to clear with the company by May 11, a move the employees say is against the existing court orders.\n",
            "\n",
            "4. The 184 moderators sued Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March.\n",
            "\n",
            "5. The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy.\n",
            "\n",
            "6. Sama was directed to continue reviewing content on Meta’s platforms, and to be its sole provider in Africa pending determination of the case. However, Sama sent the moderators on compulsory leave in April saying it had no work for them as its contract with Meta had expired.\n",
            "\n",
            "7. Sama said it sent the notice \"to staff whose contract had expired to go through our regular clearance process.\"\n",
            "\n",
            "8. Sama said the moderators' contracts had ended in March after its deal with Meta expired, and that it was only processing the moderators' final dues.\n",
            "\n",
            "9. \"We understand our former employees' frustration because they were led by others to believe that they would all receive salary indefinitely while on leave, but that is not what the court dictated,\" said Sama.\n",
            "CPU times: user 1min 10s, sys: 62.8 ms, total: 1min 10s\n",
            "Wall time: 1min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.attn_config['attn_impl']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "deQu-din3mhP",
        "outputId": "c300b317-6057-4163-c4ff-e0d193677300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1K71mYsBx1r4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e519161298c1403083cc8668064abcf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f5425ca85374bd0b0f505e9d5fa3d96",
              "IPY_MODEL_63705f9227f741f4a2008eb7e66fe357",
              "IPY_MODEL_6fbc58cb77cf4d1a82e9f28cc95ffaaa"
            ],
            "layout": "IPY_MODEL_9fc95d12b7a44798bfbfbd3abbc12caf"
          }
        },
        "1f5425ca85374bd0b0f505e9d5fa3d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e382aa10c1b4365b58e26415973a324",
            "placeholder": "​",
            "style": "IPY_MODEL_48f53d1afc1049dda7ed1f31ba0a6246",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "63705f9227f741f4a2008eb7e66fe357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96b6d06203644a0a8aee02b37622331e",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09358c12124f4a17b1a234991417255c",
            "value": 7
          }
        },
        "6fbc58cb77cf4d1a82e9f28cc95ffaaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d724ed818b428185c4fc5db50bdb90",
            "placeholder": "​",
            "style": "IPY_MODEL_9cb38479c5814a189182206fce98e859",
            "value": " 7/7 [00:49&lt;00:00,  5.76s/it]"
          }
        },
        "9fc95d12b7a44798bfbfbd3abbc12caf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e382aa10c1b4365b58e26415973a324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f53d1afc1049dda7ed1f31ba0a6246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96b6d06203644a0a8aee02b37622331e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09358c12124f4a17b1a234991417255c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82d724ed818b428185c4fc5db50bdb90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb38479c5814a189182206fce98e859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}