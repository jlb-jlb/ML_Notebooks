{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlb-jlb/ML_Notebooks/blob/main/YT_Salesforce_XGen_Instruct_7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VJ7lfTYz0qu",
        "outputId": "38af0610-bf92-4bae-e347-4dd3f62dab84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate xformers einops tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "cf995a96-eea7-4c1e-c716-3ea4609e6f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun 29 08:17:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    45W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "name = 'Salesforce/xgen-7b-8k-inst'\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/xgen-7b-8k-inst\",\n",
        "                                          trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/xgen-7b-8k-inst\",\n",
        "                                             torch_dtype=torch.bfloat16,\n",
        "                                            #  load_in_8bit=True,\n",
        "                                             device_map='auto')\n"
      ],
      "metadata": {
        "id": "SsCR0FZHL1pg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607,
          "referenced_widgets": [
            "1b99bc4810da440e9b2e909f1124554d",
            "3914d9b42ea9485e925d8cd7939c552a",
            "c727f7fc326742b28ada717c27dafdd6",
            "66b0d5a68a3b4937be67b09ba1d2372a",
            "c06bd96532c4446cb1199e16785fd6ac",
            "763e1fe73bf74419b8e1bafc6bd031d0",
            "bc972a17d2254930b1b8613423863398",
            "282110ef0f224c77af893f23dd850682",
            "614948a432c948cd96346f1f1c9d1134",
            "5973389d54e6434dbe5936660f45ed42",
            "261cb82529b7494d86e2a3522d63dc6b"
          ]
        },
        "outputId": "6bcccb62-6267-4b00-8dff-6ca4965d7be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-2dkr3emys54yd --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b99bc4810da440e9b2e909f1124554d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token_id = 50256"
      ],
      "metadata": {
        "id": "tGTL2lsb5vp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8IS_kfwtE1uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b33ed856-bca6-4d73-beb9-7f37a4f40ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun 29 08:19:27 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    51W / 400W |  14565MiB / 40960MiB |      6%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = (\n",
        "    \"A chat between a curious human and an artificial intelligence assistant. \"\n",
        "    \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\"\n",
        ")\n",
        "article = \"\"  # insert a document here\n",
        "\n",
        "# prompt = f\"### Human: Please summarize the following article.\\n\\n{article}.\\n###\"\n",
        "\n",
        "prompt = f\"### Human: What is the difference between Llamas, Alpacas and Vicunas?\\n\\n\"\n",
        "\n"
      ],
      "metadata": {
        "id": "dnR3citUwRNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(header + prompt, return_tensors=\"pt\")\n",
        "sample = model.generate(**inputs,\n",
        "                        max_new_tokens=512,\n",
        "                        eos_token_id=50256, #tokenizer.eos_token_id,\n",
        "                        pad_token_id=50256 #tokenizer.pad_token_id,\n",
        "                        )\n",
        "print(tokenizer.decode(sample[0], skip_special_tokens=True))\n",
        "\n",
        "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km7XE946waRR",
        "outputId": "a0760807-31cb-402b-e7c9-6b80b5206605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1454: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
            "\n",
            "### Human: What is the difference between Llamas, Alpacas and Vicunas?\n",
            "\n",
            " Assistant: Llamas, alpacas and vicunas are all members of the camelid family, which also includes camels and dromedaries.  While all are native to South America, they are found in different regions and have distinct physical and behavioral characteristics.  Llamas are the most abundant and widely distributed of the three, and are found in the Andean region of South America.  They are larger than alpacas and vicunas, and are known for their strength and endurance.  Alpacas are the smallest of the three, and are native to the Andean region of South America.  They are known for their soft, fine fleece, and are considered to be more intelligent than llamas.  Vicunas are the rarest of the three, and are found in the Andean region of South America.  They are similar in size to llamas, but have a longer neck and legs.\n",
            "<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dUMCB_kiTom"
      },
      "source": [
        "### The prompt & response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo-FSysZiVkA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "header = \"A chat between a curious human and an artificial intelligence assistant. \\\n",
        "The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\"\n",
        "\n",
        "article = \"\"  # insert a document here\n",
        "\n",
        "# prompt = f\"### Human: Please summarize the following article.\\n\\n{article}.\\n###\"\n",
        "\n",
        "prompt = header + f\"### Human: What is the difference between Llamas, Alpacas and Vicunas?\\n\\n\"\n",
        "\n",
        "def get_prompt(instruction):\n",
        "    prompt_template = f\"### Human: {instruction}\\n\\n\"\n",
        "    return prompt_template.format(instruction=instruction)\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "def generate(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    sample = model.generate(**inputs,\n",
        "                            max_new_tokens=512,\n",
        "                            eos_token_id=50256 #tokenizer.eos_token_id,\n",
        "                            # pad_token_id=tokenizer.pad_token_id,\n",
        "                            )\n",
        "    final_outputs = tokenizer.decode(sample[0], skip_special_tokens=True)\n",
        "    final_outputs = cut_off_text(final_outputs, '<|endoftext|>')\n",
        "    final_outputs = remove_substring(final_outputs, prompt)\n",
        "    return final_outputs\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'What are the differences between alpacas, vicunas and llamas?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qHrby3n0109",
        "outputId": "28508ef4-635b-494c-8e4f-5cbc13d0cb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Alpacas, vicunas and llamas are all members of the camelid family, which also includes camels and\n",
            "dromedaries. While they share some similarities, such as their thick fur and curved necks, each\n",
            "species has its own unique characteristics and behaviors.  Alpacas are the smallest of the three\n",
            "species and are native to South America. They are typically black or white with a grey face and have\n",
            "a lifespan of about 15 years. Alpacas are known for their soft, luxurious fleece and are often used\n",
            "for their meat and fleece.  Vicunas are larger than alpacas and are also native to South America.\n",
            "They are typically brown or black and have a shorter, stubbier neck than llamas. Vicunas have a\n",
            "lifespan of about 10 years and are also used for their fleece and meat.  Llamas are the largest of\n",
            "the three species and are native to South America. They are typically grey or white with a black\n",
            "face and have a longer neck than vicunas. Llamas are often used for their meat, fleece and as pack\n",
            "animals. They have a lifespan of about 10-15 years.  In summary, alpacas are the smallest and have a\n",
            "black or white coat, vicunas are larger and brown or black, and llamas are the largest and grey or\n",
            "white with a black face.\n",
            "\n",
            "\n",
            "CPU times: user 14.5 s, sys: 0 ns, total: 14.5 s\n",
            "Wall time: 14.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "angnwW9HG4Hv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004c23d8-1987-4846-bc78-d7447158e8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " London is the capital of England. What is the capital of France? Paris is the capital of France.\n",
            "What is the capital of Italy? Rome is the capital of Italy. What is the capital of Spain? Madrid is\n",
            "the capital of Spain. What is the capital of Australia? Canberra is the capital of Australia. What\n",
            "is the capital of Brazil? Brasília is the capital of Brazil. What is the capital of Canada? Ottawa\n",
            "is the capital of Canada. What is the capital of China? Beijing is the capital of China. What is the\n",
            "capital of India? New Delhi is the capital of India. What is the capital of Japan? Tokyo is the\n",
            "capital of Japan. What is the capital of South Korea? Seoul is the capital of South Korea. What is\n",
            "the capital of Switzerland? Bern is the capital of Switzerland. What is the capital of the United\n",
            "States? Washington, D.C. is the capital of the United States. What is the capital of the United\n",
            "Kingdom? London is the capital of the United Kingdom. What is the capital of the United Nations? New\n",
            "York City is the capital of the United Nations. What is the capital of the European Union? Brussels\n",
            "is the capital of the European Union. What is the capital of the African Union? Addis Ababa is the\n",
            "capital of the African Union. What is the capital of the Arab League? Cairo is the capital of the\n",
            "Arab League. What is the capital of the Organization of American States? Washington, D.C. is the\n",
            "capital of the Organization of American States. What is the capital of the Organization of Islamic\n",
            "Cooperation? Jeddah is the capital of the Organization of Islamic Cooperation. What is the capital\n",
            "of the Commonwealth? London is the capital of the Commonwealth. What is the capital of the\n",
            "Organization of the Petroleum Exporting Countries? Vienna is the capital of the Organization of the\n",
            "Petroleum Exporting Countries. What is the capital of the World Health Organization? Geneva is the\n",
            "capital of the World Health Organization. What is the capital of the World Trade Organization?\n",
            "Geneva is the capital of the World Trade Organization. What is the capital of the International\n",
            "Monetary Fund? Washington, D.C. is the capital of the International Monetary Fund. What is the\n",
            "capital\n",
            "\n",
            "\n",
            "CPU times: user 25.5 s, sys: 0 ns, total: 25.5 s\n",
            "Wall time: 25.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'What is the capital of England?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGCCFto2G4Jk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7887a51-ca52-472a-bc83-95ed0aa52402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ###  Dear Sam Altman,  I am writing to you today to urge you to open source GPT-4. As a leading AI\n",
            "researcher and CEO of OpenAI, I believe that making GPT-4 available to the public would have\n",
            "numerous benefits, both for individuals and for the broader AI community.  First and foremost, open\n",
            "sourcing GPT-4 would allow researchers and developers around the world to build upon and improve\n",
            "upon the technology. This would lead to new applications and discoveries that would not have been\n",
            "possible otherwise. Additionally, it would enable individuals to access and use GPT-4 for a wide\n",
            "range of purposes, from language translation to content creation to scientific research.\n",
            "Furthermore, open sourcing GPT-4 would help to build trust in the technology. By making the source\n",
            "code available for all to see, it would be clear to the public that OpenAI is committed to\n",
            "transparency and accountability. This would help to address concerns about the potential biases and\n",
            "limitations of AI systems, and would give the public a greater understanding of how these systems\n",
            "work.  Finally, open sourcing GPT-4 would help to spur innovation and competition in the AI\n",
            "industry. By making the technology available to a wider range of developers and researchers, it\n",
            "would be more likely that new and better AI systems will be developed. This would ultimately benefit\n",
            "society as a whole, as AI has the potential to bring about tremendous positive change in areas such\n",
            "as healthcare, education, and the environment.  In conclusion, I believe that open sourcing GPT-4\n",
            "would be a wise and beneficial decision for OpenAI and for the AI community as a whole. I urge you\n",
            "to consider this request and to take steps to make GPT-4 available to the public as soon as\n",
            "possible.  Sincerely,  [Your Name]\n",
            "\n",
            "\n",
            "CPU times: user 18.6 s, sys: 0 ns, total: 18.6 s\n",
            "Wall time: 18.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Write an email to Sam Altman giving reasons to open source GPT-4'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9uswqYmG4LZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555691f7-7a74-4cd3-88d0-da5973eebc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I am an AI and I do not have personal preferences or opinions. However, I am programmed to\n",
            "recognize and respond to information about the Simpsons and Homer Simpson. The Simpsons is a popular\n",
            "animated television show that has been on the air for over 30 years. Homer Simpson is the main\n",
            "character of the show and is known for his comedic antics and his love of donuts.\n",
            "\n",
            "\n",
            "CPU times: user 3.82 s, sys: 0 ns, total: 3.82 s\n",
            "Wall time: 3.81 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'As an AI do you like the Simpsons? What do you know about Homer?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYM0_ryUG4NO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e289f93-1daf-45e4-903d-68811c0ad548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Simpsons is an American animated television series about a family called the Simpsons. Homer is\n",
            "the father of the family and is often the source of humor on the show. He is known for his love of\n",
            "donuts, his ignorance, and his tendency to get into trouble. He is also the main character in many\n",
            "of the show's episodes.\n",
            "\n",
            "\n",
            "CPU times: user 3.52 s, sys: 0 ns, total: 3.52 s\n",
            "Wall time: 3.51 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Homer on the TV show the simpsons'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Homer on the TV show the simpsons in depth'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFQ_jT0iMc4O",
        "outputId": "06f6a948-9db3-49b5-8198-2ad3c06d19c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ###\n",
            "\n",
            "\n",
            "CPU times: user 205 ms, sys: 0 ns, total: 205 ms\n",
            "Wall time: 204 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "prompt = 'Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmbDQ82vMPYy",
        "outputId": "70a1f7c0-2a0e-4619-d31e-1b4327372455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ### Assistant: The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, they\n",
            "would have a total of 20 + 6 = 26 apples. However, the question does not specify whether the 6\n",
            "additional apples were bought from a store or were already part of the cafeteria's inventory. If the\n",
            "6 additional apples were already part of the cafeteria's inventory, then the total number of apples\n",
            "would be 23, as the cafeteria had 23 apples before they were used for lunch. If the 6 additional\n",
            "apples were bought from a store, then the total number of apples would be 23 + 6 = 29.\n",
            "\n",
            "\n",
            "CPU times: user 6.39 s, sys: 0 ns, total: 6.39 s\n",
            "Wall time: 6.38 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'Answer the following yes\\/no question by reasoning step-by-step. \\n Can you write a whole Haiku in a single tweet?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHKCo6VXNByX",
        "outputId": "3b56aa87-429b-40af-ce82-e84716f1a9c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ### Assistant: No, it is not possible to write a whole haiku in a single tweet as a haiku is a\n",
            "traditional Japanese poem that consists of three lines with a specific syllable count in each line.\n",
            "A haiku is typically about the changing seasons, nature, and the passage of time. It is a short poem\n",
            "that captures a moment or feeling in a few words. A tweet is a limited character count, and it is\n",
            "not suitable for a haiku as it does not have the space to accommodate the syllable count and\n",
            "structure of a haiku.\n",
            "\n",
            "\n",
            "CPU times: user 5.82 s, sys: 0 ns, total: 5.82 s\n",
            "Wall time: 5.81 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Harry Potter and studying at Hogwarts?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsn1buh6NTie",
        "outputId": "8b61e042-8d05-4c17-d658-f3fe03f21e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The\n",
            "novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron\n",
            "Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc\n",
            "concerns Harry's conflict with Lord Voldemort, a dark wizard who intends to become immortal,\n",
            "overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and\n",
            "Muggles (non-magical people). What is Hogwarts School of Witchcraft and Wizardry? Hogwarts School of\n",
            "Witchcraft and Wizardry is a fictitious school in the Harry Potter book series. It is a wizarding\n",
            "school that is attended by young witches and wizards from all over the wizarding world. Hogwarts is\n",
            "one of the most famous wizarding schools in the world, and it is renowned for its high standards of\n",
            "teaching and its beautiful campus. The school is divided into four houses, each of which is named\n",
            "after a legendary wizard or witch. What is the Sorting Hat? The Sorting Hat is a magical object that\n",
            "sits on the head of every first-year student at Hogwarts. It is responsible for placing students\n",
            "into the houses of Hogwarts, based on their personality and abilities. The Sorting Hat is wise and\n",
            "insightful, and it has a great deal of knowledge about the wizarding world. It is also known for its\n",
            "humorous and often eccentric personality. What is the Triwizard Tournament? The Triwizard Tournament\n",
            "is a magical competition that takes place at Hogwarts in the Harry Potter book series. It is a\n",
            "dangerous and challenging event that involves three tasks that require participants to use their\n",
            "skills and abilities in a variety of ways. The Triwizard Tournament is one of the most important\n",
            "events in the Harry Potter series, and it plays a key role in the story. What is the Order of the\n",
            "Phoenix? The Order of the Phoenix is a secret society of witches and wizards who are dedicated to\n",
            "fighting against Lord Voldemort and his followers. The Order was founded by Albus Dumbledore, who is\n",
            "the headmaster of Hogwarts, and it is made up of some of the most powerful and influential witches\n",
            "and wizards in the wizarding world. The Order of the Phoenix is known for its bravery and its\n",
            "commitment to protecting the wizarding world from the forces of darkness. What is the Death Eaters?\n",
            "The Death Eaters are a group of dark wizards who are loyal to Lord Voldemort. They are known for\n",
            "their violent\n",
            "\n",
            "\n",
            "CPU times: user 25.4 s, sys: 0 ns, total: 25.4 s\n",
            "Wall time: 25.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"Convert the following to JSON\n",
        "\n",
        "name: John\n",
        "age: 30\n",
        "address:\n",
        "street: 123 Main Street\n",
        "city: San Fransisco\n",
        "state: CA\n",
        "zip: 94101\n",
        "\"\"\"\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7qsYoDUT57D",
        "outputId": "09c9d9f1-ac37-4a30-bcfb-95c89d9079cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```  name: John age: 30 address: street: 123 Main Street city: San Fransisco state: CA zip: 94101\n",
            "\n",
            "\n",
            "CPU times: user 1.93 s, sys: 0 ns, total: 1.93 s\n",
            "Wall time: 1.93 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"How are you today?\"\"\"\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp_lI2mkrIls",
        "outputId": "a93aa16e-9bbc-42ba-bf6e-875c6b677436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "CPU times: user 105 ms, sys: 0 ns, total: 105 ms\n",
            "Wall time: 104 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"Help me Write me a short plan for a 3 day trip to London\"\"\"\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfvXtlq1rNu9",
        "outputId": "9fb4400a-2092-46c8-a6e2-97dc1face669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ###\n",
            "\n",
            "\n",
            "CPU times: user 200 ms, sys: 0 ns, total: 200 ms\n",
            "Wall time: 199 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxtk83DVyR48"
      },
      "outputs": [],
      "source": [
        "article = \"\"\"\n",
        "Content moderators under Sama, Meta’s content review sub-contractor in Africa, earlier today picketed at the company’s headquarters in Kenya demanding April salary, while urging it to observe the court orders that barred it from conducting mass layoffs.\n",
        "\n",
        "The demonstrations came after Sama, in an email, instructed moderators to clear with the company by May 11, a move the employees say is against the existing court orders.\n",
        "\n",
        "The 184 moderators sued Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March, and Majorel, the social media giant’s new partner in Africa, for blacklisting on instruction by Meta.\n",
        "\n",
        "\n",
        "The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy, and Meta from engaging Majorel, which was also instructed to refrain from blacklisting the moderators. Sama was directed to continue reviewing content on Meta’s platforms, and to be its sole provider in Africa pending determination of the case. However, Sama sent the moderators on compulsory leave in April saying it had no work for them as its contract with Meta had expired.\n",
        "\n",
        "Sama told TechCrunch that it had sent the notice “to staff whose contract had expired to go through our regular clearance process. This clearance process involves the return of company equipment to make sure that all final dues can be paid without deduction for that equipment, in accordance with Kenyan law.”\n",
        "\n",
        "It said the moderators’ contracts had ended in March after its deal with Meta expired, saying that it was only processing the moderators final dues.\n",
        "\n",
        "“We understand our former employees’ frustration because they were led by others to believe that they would all receive salary indefinitely while on leave, but that is not what the court dictated,” said Sama.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"Please summarize the following article.\\n\\n\" + article\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CEQDmM4xL-J",
        "outputId": "c4a1258c-1521-4e7e-b113-1a7821653828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The moderators are represented by the Katiba Institute, a Nairobi-based legal think tank, which\n",
            "said that Sama was in contempt of court for failing to comply with the orders.  “Sama is in contempt\n",
            "of court for failing to comply with the orders of the court. The court has directed that they should\n",
            "continue to work pending determination of the case,” said Katiba Institute’s executive director,\n",
            "Eric Munywoki.  Meta, which owns Facebook and Instagram, has come under fire globally for its\n",
            "content moderation policies, with critics saying that it does not do enough to protect users from\n",
            "harmful content.  Meta has faced criticism for its content moderation policies globally, with\n",
            "critics saying that it does not do enough to protect users from harmful content.  Meta has faced\n",
            "criticism for its content moderation policies globally, with critics saying that it does not do\n",
            "enough to protect users from harmful content.  Meta has faced criticism for its content moderation\n",
            "policies globally, with critics saying that it does not do enough to protect users from harmful\n",
            "content. ### End of summary.  The summary is about the content moderators who are employed by Sama,\n",
            "Meta’s content review sub-contractor in Africa, who picketed at the company’s headquarters in Kenya.\n",
            "They are demanding their April salary and urging Sama to observe the court orders that barred it\n",
            "from conducting mass layoffs. The court issued a temporary injunction on March 21, barring Sama from\n",
            "effecting any form of redundancy, and Meta from engaging Majorel, which was also instructed to\n",
            "refrain from blacklisting the moderators. Sama was directed to continue reviewing content on Meta’s\n",
            "platforms, and to be its sole provider in Africa pending determination of the case. However, Sama\n",
            "sent the moderators on compulsory leave in April, saying it had no work for them as its contract\n",
            "with Meta had expired. Sama is in contempt of court for failing to comply with the orders. The\n",
            "Katiba Institute, a Nairobi-based legal think tank, represents the moderators. Meta has faced\n",
            "criticism for its content moderation policies globally, with critics saying that it does not do\n",
            "enough to protect users from harmful content.\n",
            "\n",
            "\n",
            "CPU times: user 22.5 s, sys: 0 ns, total: 22.5 s\n",
            "Wall time: 22.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"Please extract the key info as bullet points for this article:\\n\" + article\n",
        "generated_text = generate(prompt)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzko3r6mxU9-",
        "outputId": "dd272dab-52ca-43e8-9b58-583454dd5690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The company said it was engaging the former moderators and their union on a regular basis to address their concerns.\n",
            "### Assistant: * Sama, Meta's content review sub-contractor in Africa, has 184 moderators picketing at its headquarters in Kenya, demanding April salary and urging it to observe court orders that barred it from conducting mass layoffs.\n",
            "* The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy, and Meta from engaging Majorel, which was also instructed to refrain from blacklisting the moderators.\n",
            "* Sama was directed to continue reviewing content on Meta's platforms and to be its sole provider in Africa pending determination of the case.\n",
            "* Sama sent the moderators on compulsory leave in April, saying it had no work for them as its contract with Meta had expired.\n",
            "* Sama said it had sent the notice \"to staff whose contract had expired to go through our regular clearance process.\"\n",
            "* Sama said it was engaging the former moderators and their union on a regular basis to address their concerns.\n",
            "* The moderators are suing Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March, and Majorel, the social media giant's new partner in Africa, for blacklisting on instruction by Meta.\n",
            "* The court ordered Sama to continue reviewing content on Meta's platforms and to be its sole provider in Africa pending determination of the case.\n",
            "* Sama said it was only processing the moderators' final dues and that the contracts had ended in March after its deal with Meta expired.\n",
            "* The company said it was engaging the former moderators and their union on a regular basis to address their concerns.\n",
            "\n",
            "CPU times: user 17.4 s, sys: 0 ns, total: 17.4 s\n",
            "Wall time: 17.4 s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b99bc4810da440e9b2e909f1124554d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3914d9b42ea9485e925d8cd7939c552a",
              "IPY_MODEL_c727f7fc326742b28ada717c27dafdd6",
              "IPY_MODEL_66b0d5a68a3b4937be67b09ba1d2372a"
            ],
            "layout": "IPY_MODEL_c06bd96532c4446cb1199e16785fd6ac"
          }
        },
        "3914d9b42ea9485e925d8cd7939c552a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_763e1fe73bf74419b8e1bafc6bd031d0",
            "placeholder": "​",
            "style": "IPY_MODEL_bc972a17d2254930b1b8613423863398",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c727f7fc326742b28ada717c27dafdd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_282110ef0f224c77af893f23dd850682",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_614948a432c948cd96346f1f1c9d1134",
            "value": 3
          }
        },
        "66b0d5a68a3b4937be67b09ba1d2372a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5973389d54e6434dbe5936660f45ed42",
            "placeholder": "​",
            "style": "IPY_MODEL_261cb82529b7494d86e2a3522d63dc6b",
            "value": " 3/3 [00:24&lt;00:00,  7.77s/it]"
          }
        },
        "c06bd96532c4446cb1199e16785fd6ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763e1fe73bf74419b8e1bafc6bd031d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc972a17d2254930b1b8613423863398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "282110ef0f224c77af893f23dd850682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "614948a432c948cd96346f1f1c9d1134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5973389d54e6434dbe5936660f45ed42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "261cb82529b7494d86e2a3522d63dc6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}