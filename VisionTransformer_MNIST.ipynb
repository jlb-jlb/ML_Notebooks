{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMVI1amPdmG3nqWvtd/aKQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlb-jlb/ML_Notebooks/blob/main/VisionTransformer_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from tqdm import tqdm, trange"
      ],
      "metadata": {
        "id": "DoF5ut4-t8Kp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XwQNrcqt4_4",
        "outputId": "3ee2e26d-5677-4d06-84d8-4c3cd11fb53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./../datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 131831117.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./../datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./../datasets/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 21751785.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./../datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 180765085.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./../datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 7580791.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw\n",
            "\n",
            "Using device:  cuda (Tesla T4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/10 [00:00<?, ?it/s]\n",
            "Epoch 1 in training:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1 in training:   0%|          | 1/469 [00:03<29:11,  3.74s/it]\u001b[A\n",
            "Epoch 1 in training:   0%|          | 2/469 [00:04<16:03,  2.06s/it]\u001b[A\n",
            "Epoch 1 in training:   1%|          | 3/469 [00:05<10:50,  1.40s/it]\u001b[A\n",
            "Epoch 1 in training:   1%|          | 4/469 [00:05<08:29,  1.10s/it]\u001b[A\n",
            "Epoch 1 in training:   1%|          | 5/469 [00:06<07:08,  1.08it/s]\u001b[A\n",
            "Epoch 1 in training:   1%|▏         | 6/469 [00:07<06:22,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:   1%|▏         | 7/469 [00:07<05:53,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:   2%|▏         | 8/469 [00:08<05:30,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:   2%|▏         | 9/469 [00:09<05:36,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:   2%|▏         | 10/469 [00:09<05:22,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   2%|▏         | 11/469 [00:10<05:11,  1.47it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 12/469 [00:11<05:03,  1.51it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 13/469 [00:11<04:54,  1.55it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 14/469 [00:12<04:50,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 15/469 [00:12<04:46,  1.58it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 16/469 [00:13<04:41,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▎         | 17/469 [00:14<04:40,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▍         | 18/469 [00:14<04:48,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▍         | 19/469 [00:16<06:47,  1.10it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▍         | 20/469 [00:17<06:33,  1.14it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▍         | 21/469 [00:17<05:58,  1.25it/s]\u001b[A\n",
            "Epoch 1 in training:   5%|▍         | 22/469 [00:18<05:36,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:   5%|▍         | 23/469 [00:18<05:16,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:   5%|▌         | 24/469 [00:19<05:04,  1.46it/s]\u001b[A\n",
            "Epoch 1 in training:   5%|▌         | 25/469 [00:20<04:54,  1.51it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▌         | 26/469 [00:20<04:48,  1.53it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▌         | 27/469 [00:21<04:46,  1.54it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▌         | 28/469 [00:22<04:41,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▌         | 29/469 [00:22<04:38,  1.58it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▋         | 30/469 [00:23<04:35,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 31/469 [00:23<04:31,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 32/469 [00:24<04:30,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 33/469 [00:25<04:48,  1.51it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 34/469 [00:25<04:41,  1.54it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 35/469 [00:26<04:35,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:   8%|▊         | 36/469 [00:27<04:53,  1.47it/s]\u001b[A\n",
            "Epoch 1 in training:   8%|▊         | 37/469 [00:28<05:20,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:   8%|▊         | 38/469 [00:29<05:34,  1.29it/s]\u001b[A\n",
            "Epoch 1 in training:   8%|▊         | 39/469 [00:29<05:13,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▊         | 40/469 [00:30<04:58,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▊         | 41/469 [00:30<04:46,  1.50it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▉         | 42/469 [00:31<04:39,  1.53it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▉         | 43/469 [00:32<04:33,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▉         | 44/469 [00:32<04:30,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|▉         | 45/469 [00:33<04:26,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|▉         | 46/469 [00:33<04:23,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|█         | 47/469 [00:34<04:22,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|█         | 48/469 [00:35<04:20,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|█         | 49/469 [00:35<04:20,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  11%|█         | 50/469 [00:36<04:19,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  11%|█         | 51/469 [00:37<04:17,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  11%|█         | 52/469 [00:37<04:18,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  11%|█▏        | 53/469 [00:38<04:17,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 54/469 [00:38<04:16,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 55/469 [00:39<04:52,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 56/469 [00:40<05:15,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 57/469 [00:41<05:26,  1.26it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 58/469 [00:42<05:04,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 59/469 [00:42<04:50,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 60/469 [00:43<04:37,  1.47it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 61/469 [00:44<04:30,  1.51it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 62/469 [00:44<04:25,  1.54it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 63/469 [00:45<04:19,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▎        | 64/469 [00:45<04:19,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▍        | 65/469 [00:46<04:15,  1.58it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▍        | 66/469 [00:47<04:15,  1.58it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▍        | 67/469 [00:47<04:13,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▍        | 68/469 [00:48<04:10,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  15%|█▍        | 69/469 [00:49<04:11,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  15%|█▍        | 70/469 [00:49<04:13,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:  15%|█▌        | 71/469 [00:50<04:10,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  15%|█▌        | 72/469 [00:50<04:09,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▌        | 73/469 [00:51<04:41,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▌        | 74/469 [00:52<05:00,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▌        | 75/469 [00:53<05:01,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▌        | 76/469 [00:54<04:45,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▋        | 77/469 [00:54<04:30,  1.45it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 78/469 [00:55<04:23,  1.49it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 79/469 [00:56<04:17,  1.52it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 80/469 [00:56<04:10,  1.55it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 81/469 [00:57<04:08,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 82/469 [00:57<04:03,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  18%|█▊        | 83/469 [00:58<04:01,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  18%|█▊        | 84/469 [00:59<03:59,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  18%|█▊        | 85/469 [00:59<03:57,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  18%|█▊        | 86/469 [01:00<03:59,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▊        | 87/469 [01:00<03:59,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▉        | 88/469 [01:01<03:58,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▉        | 89/469 [01:02<03:57,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▉        | 90/469 [01:02<03:55,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▉        | 91/469 [01:03<04:10,  1.51it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|█▉        | 92/469 [01:04<04:37,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|█▉        | 93/469 [01:05<04:56,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|██        | 94/469 [01:06<04:38,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|██        | 95/469 [01:06<04:25,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|██        | 96/469 [01:07<04:15,  1.46it/s]\u001b[A\n",
            "Epoch 1 in training:  21%|██        | 97/469 [01:07<04:06,  1.51it/s]\u001b[A\n",
            "Epoch 1 in training:  21%|██        | 98/469 [01:08<04:02,  1.53it/s]\u001b[A\n",
            "Epoch 1 in training:  21%|██        | 99/469 [01:09<03:58,  1.55it/s]\u001b[A\n",
            "Epoch 1 in training:  21%|██▏       | 100/469 [01:09<03:54,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 101/469 [01:10<03:52,  1.58it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 102/469 [01:11<03:51,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 103/469 [01:11<03:49,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 104/469 [01:12<03:48,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 105/469 [01:12<03:45,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 106/469 [01:13<03:44,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 107/469 [01:14<03:43,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 108/469 [01:14<03:42,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 109/469 [01:15<03:44,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 110/469 [01:16<04:18,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  24%|██▎       | 111/469 [01:17<04:37,  1.29it/s]\u001b[A\n",
            "Epoch 1 in training:  24%|██▍       | 112/469 [01:17<04:31,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  24%|██▍       | 113/469 [01:18<04:15,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  24%|██▍       | 114/469 [01:19<04:03,  1.46it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▍       | 115/469 [01:19<03:57,  1.49it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▍       | 116/469 [01:20<03:51,  1.52it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▍       | 117/469 [01:21<03:47,  1.54it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▌       | 118/469 [01:21<03:44,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▌       | 119/469 [01:22<03:40,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▌       | 120/469 [01:22<03:37,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▌       | 121/469 [01:23<03:37,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▌       | 122/469 [01:24<03:35,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▌       | 123/469 [01:24<03:36,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▋       | 124/469 [01:25<03:33,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  27%|██▋       | 125/469 [01:26<03:32,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  27%|██▋       | 126/469 [01:26<03:40,  1.55it/s]\u001b[A\n",
            "Epoch 1 in training:  27%|██▋       | 127/469 [01:27<03:38,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:  27%|██▋       | 128/469 [01:28<03:53,  1.46it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 129/469 [01:29<04:46,  1.19it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 130/469 [01:30<04:57,  1.14it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 131/469 [01:30<04:30,  1.25it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 132/469 [01:31<04:13,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 133/469 [01:32<03:58,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▊       | 134/469 [01:32<03:47,  1.47it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▉       | 135/469 [01:33<03:41,  1.51it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▉       | 136/469 [01:34<03:36,  1.54it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▉       | 137/469 [01:34<03:31,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▉       | 138/469 [01:35<03:28,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|██▉       | 139/469 [01:35<03:27,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|██▉       | 140/469 [01:36<03:25,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|███       | 141/469 [01:37<03:24,  1.60it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|███       | 142/469 [01:37<03:22,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|███       | 143/469 [01:38<03:22,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  31%|███       | 144/469 [01:38<03:21,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  31%|███       | 145/469 [01:39<03:20,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  31%|███       | 146/469 [01:40<03:36,  1.49it/s]\u001b[A\n",
            "Epoch 1 in training:  31%|███▏      | 147/469 [01:41<03:59,  1.34it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 148/469 [01:42<04:12,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 149/469 [01:42<03:55,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 150/469 [01:43<03:42,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 151/469 [01:44<03:34,  1.48it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 152/469 [01:44<03:27,  1.52it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 153/469 [01:45<03:24,  1.55it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 154/469 [01:45<03:20,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 155/469 [01:46<03:20,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 156/469 [01:47<03:19,  1.57it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 157/469 [01:47<03:16,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  34%|███▎      | 158/469 [01:48<03:15,  1.59it/s]\u001b[A\n",
            "Epoch 1 in training:  34%|███▍      | 159/469 [01:48<03:12,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  34%|███▍      | 160/469 [01:49<03:12,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  34%|███▍      | 161/469 [01:50<03:10,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▍      | 162/469 [01:50<03:09,  1.62it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▍      | 163/469 [01:51<03:10,  1.61it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▍      | 164/469 [01:52<03:15,  1.56it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▌      | 165/469 [01:53<03:54,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▌      | 166/469 [01:54<04:05,  1.23it/s]\u001b[A\n",
            "Epoch 1 in training:  36%|███▌      | 167/469 [01:54<04:01,  1.25it/s]\u001b[A\n",
            "Epoch 1 in training:  36%|███▌      | 168/469 [01:55<03:45,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  36%|███▌      | 169/469 [01:56<03:32,  1.41it/s]\u001b[A"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "def patchify(images, n_patches):\n",
        "    n, c, h, w = images.shape\n",
        "\n",
        "    assert h == w, \"Patchify method is implemented for square images only\"\n",
        "\n",
        "    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches**2)\n",
        "    patch_size = h // n_patches\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        for i in range(n_patches):\n",
        "            for j in range(n_patches):\n",
        "                patch = image[\n",
        "                    :,\n",
        "                    i * patch_size : (i + 1) * patch_size,\n",
        "                    j * patch_size : (j + 1) * patch_size,\n",
        "                ]\n",
        "                patches[idx, i * n_patches + j] = patch.flatten()\n",
        "    return patches\n",
        "\n",
        "\n",
        "class MyMSA(nn.Module):\n",
        "    def __init__(self, d, n_heads=2):\n",
        "        super(MyMSA, self).__init__()\n",
        "        self.d = d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
        "\n",
        "        d_head = int(d / n_heads)\n",
        "        self.q_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.k_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.v_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.d_head = d_head\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        # Sequences has shape (N, seq_length, token_dim)\n",
        "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
        "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
        "        result = []\n",
        "        for sequence in sequences:\n",
        "            seq_result = []\n",
        "            for head in range(self.n_heads):\n",
        "                q_mapping = self.q_mappings[head]\n",
        "                k_mapping = self.k_mappings[head]\n",
        "                v_mapping = self.v_mappings[head]\n",
        "\n",
        "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
        "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
        "\n",
        "                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n",
        "                seq_result.append(attention @ v)\n",
        "            result.append(torch.hstack(seq_result))\n",
        "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
        "\n",
        "class MyViTBlock(nn.Module):\n",
        "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "        super(MyViTBlock, self).__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_d)\n",
        "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_d)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.mhsa(self.norm1(x))\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class MyViT(nn.Module):\n",
        "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
        "        # Super constructor\n",
        "        super(MyViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.chw = chw  # ( C , H , W )\n",
        "        self.n_patches = n_patches\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_d = hidden_d\n",
        "\n",
        "        # Input and patches sizes\n",
        "        assert (\n",
        "            chw[1] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by number of patches\"\n",
        "        assert (\n",
        "            chw[2] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by number of patches\"\n",
        "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
        "\n",
        "        # 1) Linear mapper\n",
        "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
        "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "        # 2) Learnable classification token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
        "\n",
        "        # 3) Positional embedding\n",
        "        self.register_buffer(\n",
        "            \"positional_embeddings\",\n",
        "            get_positional_embeddings(n_patches**2 + 1, hidden_d),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "        # 4) Transformer encoder blocks\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)]\n",
        "        )\n",
        "\n",
        "        # 5) Classification MLPk\n",
        "        self.mlp = nn.Sequential(nn.Linear(self.hidden_d, out_d), nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Dividing images into patches\n",
        "        n, c, h, w = images.shape\n",
        "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
        "\n",
        "        # Running linear layer tokenization\n",
        "        # Map the vector corresponding to each patch to the hidden size dimension\n",
        "        tokens = self.linear_mapper(patches)\n",
        "\n",
        "        # Adding classification token to the tokens\n",
        "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
        "\n",
        "        # Adding positional embedding\n",
        "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        # Getting the classification token only\n",
        "        out = out[:, 0]\n",
        "\n",
        "        return self.mlp(out)  # Map to output dimension, output category distribution\n",
        "\n",
        "\n",
        "def get_positional_embeddings(sequence_length, d):\n",
        "    result = torch.ones(sequence_length, d)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(d):\n",
        "            result[i][j] = (\n",
        "                np.sin(i / (10000 ** (j / d)))\n",
        "                if j % 2 == 0\n",
        "                else np.cos(i / (10000 ** ((j - 1) / d)))\n",
        "            )\n",
        "    return result\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Loading data\n",
        "    transform = ToTensor()\n",
        "\n",
        "    train_set = MNIST(\n",
        "        root=\"./../datasets\", train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_set = MNIST(\n",
        "        root=\"./../datasets\", train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
        "\n",
        "    # Defining model and training options\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\n",
        "        \"Using device: \",\n",
        "        device,\n",
        "        f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
        "    )\n",
        "    model = MyViT(\n",
        "        (1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10\n",
        "    ).to(device)\n",
        "    N_EPOCHS = 10 # Hier anpassen\n",
        "    LR = 0.005\n",
        "\n",
        "    # Training loop\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "    criterion = CrossEntropyLoss()\n",
        "    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(\n",
        "            train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False\n",
        "        ):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "\n",
        "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
        "\n",
        "    # Test loop\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        test_loss = 0.0\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "            total += len(x)\n",
        "        print(f\"Test loss: {test_loss:.2f}\")\n",
        "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}